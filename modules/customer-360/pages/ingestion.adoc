:imagesdir: ../assets/images

= Customer 360 Data Ingestion

This tutorial contains 7 main sectins:

* <<Customer 360 Introduction>>: What does it mean? What is the use case?
* <<Data Ingestion Overview>>: How and why are we ingesting data?
* <<Data Ingestion example with Kafka>>: Using Kafka and Kafka Connect to setup a real-time data pipeline
* <<Data Ingestion example with CSV>>: Creating a (nightly) job to import data from a CSV file
* <<Data Ingestion with HTTP REST API polling>>: Planning to use CURL/HTTP to ingest data
* <<Data Integration with Couchbase Eventing Functions>>: putting all the pieces together
* <<Summary>> and next steps: Once you have the data ingested, where do you go from there?

*Prerequisities*: this is a tutorial about a data ingestion architecture. It is not necessarily a step-by-step guide on creating everything from start to finish. There are so many variables to take into account, that it would be impossible to cover all of them. But, generally speaking, the prerequisites are:

* An enterprise with multiple data-backed systems.
* Some way for data to be copied/piped/streamed from those systems (this tutorial covers Kafka, CSV, and HTTP, but there are a myriad of other ways to move data around).
* A Couchbase Server cluster with (at least) one node that has the link:https://docs.couchbase.com/server/current/eventing/eventing-overview.html[Eventing service] enabled.
* Optional: Docker. If you plan on building an entire "toy" environment and following this tutorial exactly, Docker will help you. However, Docker is not necessary to the most crucial parts of this tutorial.

== Customer 360 Introduction

The goal of a link:https://www.couchbase.com/solutions/customer-360[Customer 360 system] is to deliver a single, consistent view of all your data in one platform within an enterprise where that data is split up between many different systems. This tutorial will be focusing mainly on getting a complete view of a customer/person.

=== Retail as an example

Let's assume our enterprise is a retail organization. Within this organization, there can be many separate components:

* Home Delivery - a system dedicated to received delivery orders from a customer and making sure they get delivered to their homes.
* Loyalty Program - a system dedicated to tracking and rewarding your most loyal customers with incentives.
* Online Store - a system which allows the users to place orders from their browser/phone/etc. A customer cam pick them up, have them shipped, etc.

There are many more systems, especially as your retail organization grows. Perhaps another system is acquired via acquisition, another is created by a successful hackathon project, and perhaps there are even 3rd party systems that are business critical. Some of these systems *may* even communicate with each other.

=== What do the parts have in common?

But what these systems all have in common is that they are separated in some way, and typically have their own data store.

image:00201-enterprise.png[Diagram of the enterprise in current state]

In this example, the home delivery system is backed by a MySQL database, the loyalty system is backed by Postgres, and we're not even sure what the online store is backed by.

What we do know is that an individual may interact with any of these systems. That is, a person might only use the home delivery system, they might only use the online store, or they might use 2 or 3 them. 

From the person's external point of view, they are all part of the same business. From an internal technology point of view, they are all separate parts. BUT, to best serve a customer and/or gather insights about our customers on the whole, we'd have to piece together data from these 3+ systems.

This is where Customer 360 comes in. We'll build a system to get as many pieces of the whole customer as we can, and put them together into one system.

== Data Ingestion Overview

The first step with data ingestion is to answer the question: "how can we get the data?" The answer to this question is going to vary wildly. But for purposes of this tutorial, let's assume that we've researched each system and discovered the most optimal method currently available:

image:00202-getting-data.png[Getting data out]

In this diagram, we've made the following determinations about the most optimal way to get data:

* *Home Delivery*: We're in luck! The Home Delivery team has agreed to (or already has) connected their MySQL database as a "source" to Kafka via Kafka Connect. We can setup Couchbase as a Kafka "sink" and start ingesting data in real time.
* *Loyalty System*: The situation is less ideal here, but the Loyalty System team has agreed to (or previously has) created a nightly job that exports all of their customer data to a CSV file. This means that the loyalty data could be as old as 24 hours, but it's a start!
* *Online Store*: Making things more tricky, the only way to access data externally in this system is via a REST API. We can ask for a data about one customer at a time. So instead of data coming to our Customer 360 system, our Custoemr 360 system has to poll for the data.

There are many other ways that data can flow or be accessed, but this tutorial is going to focus on these three. _In an ideal world, we could use a single method (like Kafka) to move data from all systems through the enterprise, but few enterprises exist in such a world._

== Data Ingestion example with Kafka

The Home Delivery team is using a MySQL database. We want to get customer data from that system into a Couchbase "staging" bucket. A simple view of this architecture will look like this:

image:00203-kafka-mysql.png[MySQL to Couchbase Kafka data pipeline]

Let's break down each of these parts:

* *MySQL*. This is a popular relational database. In the Home Delivery system, there may be many _tables_ containing data. Our Customer 360 system is only interested (for now) in the data that's in a _customer_ table. The Home Delivery system will make updates and inserts to this table on a regular basis as part of day-to-day business.
* *Kafka*. link:http://kafka.apache.org/[Kafka] is a streaming platform. It's similar to a queue: _sources_ put messages into a topic, and _sinks_ read the messages from those topics. Kafka has a distributed architecture, meaning that it can be scaled up as necessary.
* *Kafka Connect*. link:https://docs.confluent.io/current/connect/index.html[Kafka Connect] is a framework for connecting _data sources_ and _data sinks_ to Kafka. There are many Kafka Connect plugins available, including link:https://debezium.io/[Debezium for MySQL] and a link:https://docs.couchbase.com/kafka-connector/current/quickstart.html[Couchbase Kafka Connector].
* *Couchbase*. This is the data platform that will hold all the Customer 360 data. To start with, we'll just put data into a _bucket_ called _staging_. Later, we'll integrate this data into a _customer360_ bucket. Couchbase, like Kafka, has a distributed architecture, which provides excellent scaling capabilities.

For this tutorial, I used some basic Docker commands to get all the pieces working. In your enterprise, you may or may not be using some combinatino of containers, VMs, Kubernetes, bare metal, cloud services, etc. This tutorial will not go into all those details.

In your organization, you may already have Kafka running. If so, skip to <<MySQL to Kafka Connect>>. If you already have MySQL and Kafka Connect talking, you can skip to <<Kafka Connect to Couchbase>>.

=== Installing Couchbase

There are multiple ways to install Couchbase. I'm using basic Docker commands in this tutorial to keep it simple. However, for a more robust deployment, you may want to take a look at the link:https://docs.couchbase.com/operator/current/overview.html[Couchbase Kubernetes Autonomous Operator].

I executed the following Docker command to start Couchbase:

[source,PowerShell,indent=0]
----
docker run -d --name db -m 4gb -p 8091-8096:8091-8096 -p 9140:9140 -p 11210:11210 --link kafka:kafka couchbase:enterprise-6.0.1 
----

Once it was installed, I went through the link:https://docs.couchbase.com/server/current/manage/manage-nodes/create-cluster.html[normal setup process] (make sure that Eventing is selected) and created a bucket called _staging_. You will also need two other buckets later, so you may as well create them now: _customer360_ and _customer360_metadata_.

=== Kafka

For this tutorial, I relied heavily on Kafka and Debezium documentation. Debezium has published a link:https://debezium.io/docs/tutorial/[quick start tutorial] on setting up Kafka and Kafka connect (including Zookeeper). It's a great tutorial, and since I'll be using Debezium's MySQL connector anyway, it's a great place to start.

To get started with Kafka, I ran Docker images like so:

[source,PowerShell,indent=0]
----
docker run -it --rm --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 debezium/zookeeper:0.9

docker run -it --rm --name kafka -p 9092:9092 --link zookeeper:zookeeper debezium/kafka:0.9
----

Zookeeper is required for Kafka, so I executed that first. After that started, I ran the Kafka image. In your enterprise, I'm assuming that a MySQL database already exists, but if you want to create one just to follow along with this tutorial, you can use a Debezium provided MySQL database that already contains sample data:

[source,PowerShell,indent=0]
----
docker run -it --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9
----

At this point, you have Kafka (+Zookeeper) and MySQL running.

=== MySQL to Kafka Connect

For this tutorial, we'll be relying on the link:https://debezium.io/docs/connectors/mysql/[Debezium MySQL connector]. There are other options for using MySQL with Kafka Connect. I have chosen Debezium because of how easy it was for me to get started with it, but depending on your situation, you may want to link:https://www.confluent.io/hub/[check out other connectors].

Getting started with the Debezium MySQL connector is as easy as running another Docker image:

[source,PowerShell,indent=0]
----
docker run -it --rm --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses --link zookeeper:zookeeper --link kafka:kafka --link mysql:mysql debezium/connect:0.9
----

*However, before you do that, I would recommend reading the next step about using the Couchbase Kafka Connector.*

Once the above Connect image is running, you can configure and start the connector with a single call to Kafka Connect's REST API. Specifically, you can make a call like this:

[source,JavaScript,indent=0]
----
POST http://localhost:8083/connectors/

{
  "name": "inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "database.server.name": "dbserver1",
    "database.whitelist": "inventory",
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "schema-changes.inventory"
  }
}
----

Take special note of `database.server.name` and `database.whitelist`. After you POST this (see link:https://docs.confluent.io/current/connect/references/restapi.html#post--connectors[more about the Kafka Connect REST API here]), data will immediately start flowing from MySQL into Kafka.

=== Kafka Connect to Couchbase

At this point, data is flowing into Kafka whenever it's added or updated in MySQL. The last step is to get Couchbase to listen to this flow of data and ingest it into the _staging_ bucket.

Couchbase has created and supports a link:https://docs.couchbase.com/kafka-connector/current/quickstart.html[Kafka Connector]. Be sure to read that documentation and quick start guide. For this tutorial, we're going to put the Couchbase Kafka Connector inside the same image as used in the above step.

==== Custom Docker Image

Once again, Docker is not a prerequisite for using Couchbase Kafka Connect (or anything in this tutorial). But for convenience sake, I created a customer Docker image that has both the MySQL connector and the Couchbase connector in it:

[source,Dockerfile,indent=0]
----
include::../examples/docker-connect-couchbase/Dockerfile[]
----

To build this custom docker image on your machine, start by downloading the link:https://docs.couchbase.com/kafka-connector/current/quickstart.html#download[kafka-connect-couchbase-X.X.X.jar] file and placing in the same directory as the above Dockerfile. Then execute this Docker command:

[source,PowerShell,indent=0]
----
docker build . --tag couchbasedebezium
----

(I used `couchbasedebezium` but you're free to name it whatever you'd like).

==== Running the Custom Connect Image

Once that custom image is created, execute it like so:

[source,PowerShell,indent=0]
----
docker run -it --rm --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses --link zookeeper:zookeeper --link kafka:kafka --link mysql:mysql --link db:db couchbasedebezium
----

Notice that this is identical to running the debezium image as in the last section, except for two things:

* It's linking to db (which is where I'm running Couchbase)
* It's running my `couchbasedebezium` image

==== Starting the Connector(s)

When this image starts, you can again make REST requests to start data flowing. To start the Couchbase connector:

[source,JavaScript,indent=0]
----
POST http://localhost:8083/connectors/ 

{
  "name": "home-delivery-sink",
  "config": {
    "connector.class": "com.couchbase.connect.kafka.CouchbaseSinkConnector",
    "tasks.max": "2",
	"topics" : "dbserver1.inventory.customers",
	"connection.cluster_address" : "db",
	"connection.timeout.ms" : "2000",
	"connection.bucket" : "staging",
	"connection.username" : "Administrator",
	"connection.password" : "password",
	"couchbase.durability.persist_to" : "NONE",
	"couchbase.durability.replicate_to" : "NONE",
	"key.converter" : "org.apache.kafka.connect.storage.StringConverter",
	"value.converter" : "org.apache.kafka.connect.json.JsonConverter",
	"value.converter.schemas.enable" : "false"
  }
}
----

This configuration is largely pulled from the link:https://docs.couchbase.com/kafka-connector/3.4/quickstart.html[Couchbase Quickstart]. Instead of being a text file for the command line, it’s an HTTP POST. Take special note of:

* `connector.class` – This is the connector class that lives in the JAR file
* `topics` – The topics that Couchbase will sink from.
* `connection.cluster_address` – When I started Couchbase in Docker, I gave it a name of "db"
* `connection.bucket, connection.username, connection.password` – These are all settings I created when setting up Couchbase.

At this point, data should be flowing into your Couchbase staging bucket, and it will look similar to this:

image:00204-staging-delivery.png[Home Delivery data flowing into Couchbase]

Those documents contain a lot of information about the data flow. Some of which you might need, some of which you can ignore. But for now, the data is being ingested, and will continue to be ingested in real time as changes are made to the data in MySQL.

== Data Ingestion example with CSV

The next part of the enterprise to ingest data from is the Loyalty Program system. Since it's using Postgres, we could absolutely follow a similar procedure as was done with Kafka in the previous section. However, that's not always possible. Perhaps the Loyalty Program team doesn't want the database to connect to Kafka, perhaps they are interested in scrubbing their data first, etc.

There could be any number of reasons, but the end result is that all we have to go on is a nightly CSV export of all the customer data. Well, that's okay, because we have a few options for ingesting data from a CSV file into the _staging_ bucket in Couchbase.

=== Using cbimport

The first option is a tool that comes with Couchbase called link:https://docs.couchbase.com/server/current/tools/cbimport.html[cbimport]. This command line utility is able to import data from link:https://docs.couchbase.com/server/current/tools/cbimport-csv.html[CSV] (as well as JSON).

For example:

[source,PowerShell,indent=0]
----
cbimport csv -c couchbase://127.0.0.1 -u Administrator -p password \
 -b staging -d file:///data/loyalty_members.csv -g key::#MONO_INCR# -t 4
----

That command will import every record from loyalty_members.csv into the _staging_ bucket.

To run this nightly, you could set up a link:https://en.wikipedia.org/wiki/Cron[cron] job (or similar) to run shortly after new CSV files appear. You could schedule it or trigger it to run when a new CSV is found. This requires cbimport to have read access to the CSV file, and it requires some understanding of scripting.

Using *cbimport* is the simplest approach. It is a powerful, flexible tool that will work for you in many situations.

=== Using a Couchbase SDK

If scripting/cron/cbimport can't accomplish what you want, you can take the approach of writing your own CSV import tool. You can write a program using any of the link:https://docs.couchbase.com/server/6.0/sdk/overview.html[Couchbase Server SDKs], including C, .NET, Go, Java, Node, PHP, Python, or Scala (and there are other community supported options).

For this tutorial, I created a small program using link:https://docs.couchbase.com/dotnet-sdk/current/start-using-sdk.html[.NET] and the link:https://joshclose.github.io/CsvHelper/[CsvHelper] library.

[source,C#,indent=0]
----
include::../examples/Cust360Simulator/Cust360Simulator.Core/Loyalty/LoyaltyCsvImportService.cs[tag=ImportCsv]
----

This is a very straightforward approach: it loops through every CSV record, reads it into a C# object, and then sends it to Couchbase. It's using C#'s async/await functionality as well as the .NET SDK's batch insert capabilities. This code will also keep track of which CSV files have already been imported, so that no redundant importing happens. It also makes use of a logging library, just in case something goes wrong.

=== Time-to-live

It's also worth pointing out that this CSV import code is setting an `Expiry` of 90000 on each document. Since this is a nightly task, and *all* of the customers will be exported every day, it's important to clean up the old staging data. By setting an Expiry, Couchbase will automatically remove these temporary documents in staging after a certain period of time. link:https://docs.couchbase.com/dotnet-sdk/2.7/core-operations.html#expiry[Couchbase SDKs can set this Expiry (also know as time-to-live or TTL)] on a per document basis. You can also choose to set a TTL at the bucket level.

Since the _staging_ bucket is meant to be a place for the initial ingestion of data to appear, but NOT a place for data to be stored long term, using TTL here will save you the headache of having to clean up data.

== Data Ingestion with HTTP REST API polling

Finally, we have one more system to ingest data from. In the previous two examples, we set up data pipelines that *push* the data from their respective systems *to* Couchbase. The data comes to us.

For this next system, the only way to get data out of it is through a REST HTTP API. Additionally, we have to get data out of it piecemeal. That is, it won't simply be one request to ingest all the data at once. We can only make requests for one person at a time.

For this tutorial, I've constructed a "mock" REST API with the following endpoints:

[source,indent=0]
----
GET ​/api​/onlineStore​/getCustomerDetailsByEmail
GET ​/api​/onlineStore​/getOrdersByCustomerId
GET ​/api​/onlineStore​/getProductDetailsByProductId
----

In reality, a REST API will have a much larger suite of endpoints, including POST, DELETE, PUT, etc, as well as a variety of difference resources. But for the purposes of this tutorial, these three endpoints are all we have available that allow us to retrieve data from the Online Store system.

=== getCustomerDetailsByEmail endpoint

This endpoint expects an email address to identify the customer you want to get details about. It returns some information about the customer:

[source,JavaScript,indent=0]
----
GET /api/onlineStore/getCustomerDetailsByEmail?customerEmail=gia_kutch%40oconnell.biz

{
  "Id": 1,
  "Name": "Matt Groves",
  "Email": "matthew.groves@couchbase.com",
  "Password": "edzchzmatv5"
}
----

There's not much here about the customer, but it's certainly worth ingesting to start with.

=== getOrdersByCustomerId

This endpoint expects a customer ID (which can be obtained using the previous endpoint). It returns information about all the orders this customer has made:

[source,JavaScript,indent=0]
----
GET /api/onlineStore/getOrdersByCustomerId?customerId=1

[
  {
    "OrderId": 1,
    "PurchaseDate": "2019-04-25T15:15:5800838522",
    "CustomerId": 1,
    "Items": [
      { "OrderId": 1, "ProductId": 200, "Quantity": 2, "Price": 87.81 },
      { "OrderId": 1, "ProductId": 400, "Quantity": 4, "Price": 129.86 },
      { "OrderId": 1, "ProductId": 410, "Quantity": 1, "Price": 78.64 },
      { "OrderId": 1, "ProductId": 300, "Quantity": 2, "Price": 25.03 },
      { "OrderId": 1, "ProductId": 110, "Quantity": 2, "Price": 191.92 }
    ]
  },
  {
    "OrderId": 2,
    "PurchaseDate": "2019-04-13T16:15:5801361975",
    "CustomerId": 1,
    "Items": [
      { "OrderId": 2, "ProductId": 100, "Quantity": 2, "Price": 219.74       }
    ]
  }
]
----

The way this data is being returned indicates that a relational database of some sort is being used behind the scenes. However, much like the Loyalty system, *we don't have any direct access to that database*. We must make do with the REST endpoints for now.

There's a third endpoint, `getProductDetailsByProductId`, which will return details about a product, given a product ID. For this tutorial, I'm going to ignore this endpoint. However, it will likely become critical to ingest this information efficiently as the Customer 360 system starts to mature (in a future tutorial).

=== Ingesting this data

One approach to ingesting this data is to again use a Couchbase SDK and write a program. This program could examine our Customer 360 documents, make requests to the API, and update the Customer 360 documents (in the _customer360_ bucket) with the details that are returned. At the same time, it could gather the order information and put those into the Customer 360 bucket. In psuedo-code:

[source,indent=0]
----
for each document X in the customer360 bucket B
  email = X.email
  details = HTTP request to getCustomerDetailsByEmail
  orders = HTTP request to getOrdersByCustomerId
  update X.details
  create order document(s) in B
next X
----

This could be run on a regular basis, much like the CSV task earlier.

This is a valid approach. However, since we'll be writing code anyway, this seems like a good opportunity to write a Couchbase Eventing Function. We'll need this function to consolidate all the other data in staging anyway. Since Functions can make HTTP requests on their own, we can put all this functionality inside of the Couchbase Data Platform, together with the data.

== Data Integration with Couchbase Eventing Functions

== Summary

Ultimately, our goal may be just to build a customer 360 system while keeping the individual pieces intact. Or we may want to start to move/replace/transform these systems to use Couchbase as a primary data store. Either way, the first step is to start ingesting data into Couchbase.